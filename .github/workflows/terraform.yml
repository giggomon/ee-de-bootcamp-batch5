name: Terraform CI/CD

on:
  push:
    branches:
      - '**'     # this matches all branches, trigger the pipeline for any branch on push
      - 'main'
  pull_request:
    branches:
      - 'main' # This ensures the workflow runs for pull requests to the main branch

jobs:
  terraform:
    name: Terraform Workflow
    runs-on: ubuntu-latest

    steps:
      # Check out the repository
      - name: Checkout repository
        uses: actions/checkout@v3

      # Authenticate with GCP
      - name: Authenticate to GCP
        uses: google-github-actions/auth@v2
        with:
            credentials_json: ${{ secrets.GOOGLE_CREDENTIALS }}

      # Set up Terraform
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.6

      # Add environment variables for Snowflake
      - name: Set Snowflake variables
        run: |
          echo "TF_VAR_snowflake_account_name=${{ secrets.SNOWFLAKE_ACCOUNT_NAME }}" >> $GITHUB_ENV
          echo "TF_VAR_snowflake_user=${{ secrets.SNOWFLAKE_USER }}" >> $GITHUB_ENV
          echo "TF_VAR_snowflake_pwd=${{ secrets.SNOWFLAKE_PWD }}" >> $GITHUB_ENV
          echo "TF_VAR_snowflake_role=${{ secrets.SNOWFLAKE_ROLE }}" >> $GITHUB_ENV
          echo "TF_VAR_snowflake_wh=${{ secrets.SNOWFLAKE_WH }}" >> $GITHUB_ENV
          echo "TF_VAR_snowflake_database=${{ secrets.SNOWFLAKE_DATABASE }}" >> $GITHUB_ENV
          echo "TF_VAR_snowflake_schema=${{ secrets.SNOWFLAKE_SCHEMA }}" >> $GITHUB_ENV
          echo "TF_VAR_snowflake_org=${{ secrets.SNOWFLAKE_ORG }}" >> $GITHUB_ENV
          echo "TF_VAR_snowflake_storage_integration=${{ secrets.SNOWFLAKE_STORAGE_INTEGRATION }}" >> $GITHUB_ENV

      # Set the environment variable for sample_file_name
      - name: Set Terraform Variables
        run: | 
          echo "TF_VAR_sample_file_name=${{ secrets.SAMPLE_FILE_NAME }}" >> $GITHUB_ENV
          echo "TF_VAR_taxi_trip_raw_table=${{ secrets.TAXI_TRIP_RAW_TABLE }}" >> $GITHUB_ENV
          echo "TF_VAR_taxi_trip_staging_table=${{ secrets.TAXI_TRIP_STAGE_TABLE}}" >> $GITHUB_ENV


      # Debug Variables
      - name: Debug variables
        run: echo "Sample file name:$TF_VAR_sample_file_name"

      # Terraform Init with backend auth
      - name: Terraform Init with GCS backend
        run: |
          echo '${{ secrets.GOOGLE_CREDENTIALS }}' > /tmp/account.json
          export GOOGLE_APPLICATION_CREDENTIALS=/tmp/account.json
          terraform -chdir=terraform init \
            -backend-config="bucket=${{ secrets.GCS_BUCKET_NAME }}" \
            -backend-config="prefix=terraform/state" \
            -backend-config="credentials=/tmp/account.json"
        shell: bash

      # Terraform Validate
      - name: Terraform Validate
        run: terraform -chdir=terraform validate

      # Terraform Format (Optional)
      - name: Terraform Format
        run: terraform -chdir=terraform fmt -check

      # Terraform Plan
      -  name: Terraform Plan
         run: terraform -chdir=terraform plan

      # GCS Bucket status (only if it exists) checks
      - name: Import existing GCS bucket if it exists
        run: |
          BUCKET_NAME="${{ secrets.GCS_BUCKET_NAME }}"
          terraform -chdir=terraform import google_storage_bucket.landing_bucket "$BUCKET_NAME" || echo "Bucket not found, will be created"

      # Snowflake Schema (only if it exists) checks
      - name: Import existing Snowflake schema if it exists
        run: |
          # Attempt to import the schema only if it already exists
          SCHEMA_ID="${{ secrets.SNOWFLAKE_DATABASE }}|${{ secrets.SNOWFLAKE_SCHEMA }}"
          
          # Try to import it; ignore error if it doesn't exist
          terraform -chdir=terraform import snowflake_schema.schema "$SCHEMA_ID" || echo "Schema not found, will be created by Terraform"

      # Snowflake File format (only if it exists) checks
      - name: Import existing Snowflake file format if it exists
        run: |
          # Format: <database>|<schema>|<file_format_name>
          FILE_FORMAT_ID="${{ secrets.SNOWFLAKE_DATABASE }}|${{ secrets.SNOWFLAKE_SCHEMA }}|CSV_FORMAT"
          terraform -chdir=terraform import snowflake_file_format.csv_format "$FILE_FORMAT_ID" || echo "File format not found, will be created"


      # Terraform Apply (Only on Pushes)
      - name: Terraform Apply
        run: |
          if ! gcloud storage buckets describe gs://${{ secrets.GCS_BUCKET_NAME }} --format="value(name)" ; then
          echo "Bucket does not exist. Creating all resources..."
          terraform -chdir=terraform apply -auto-approve
          else
          echo "Bucket exists. Applying other changes..."
          # Temporarily remove the prevent_destroy lifecycle rule for the bucket
          # terraform -chdir=terraform apply -auto-approve -target=snowflake_schema.schema -target=snowflake_table.taxi_trips_raw
          terraform -chdir=terraform apply -auto-approve
          fi
        
  airflow-test:
    name: Test Airflow DAGs
    runs-on: ubuntu-latest
    needs: terraform  # This ensures the Terraform job completes first

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install apache-airflow==2.11.0
          pip install apache-airflow-providers-google==10.15.0
          pip install snowflake-connector-python>=3.0.0
          pip install apache-airflow-providers-snowflake

      - name: Test DAG syntax
        run: |
          # Test each DAG file for syntax errors
          for file in airflow/dags/*.py; do
            echo "Testing $file..."
            python -c "import sys; from airflow.models.dag import DAG; from airflow.utils.dag_cycle_tester import check_cycle; d = __import__('$file'.replace('/', '.').replace('.py', '')).dag; check_cycle(d); print('Syntax OK')"
          done

      - name: Configure Airflow Environment Variables
        run: |
          # Add Snowflake variables that your DAGs expect
          export AIRFLOW__CORE__UNIT_TEST_MODE=True
          export AIRFLOW_VAR_snowflake_account=${{ secrets.SNOWFLAKE_ACCOUNT_NAME }}
          export AIRFLOW_VAR_snowflake_user=${{ secrets.SNOWFLAKE_USER }}
          export AIRFLOW_VAR_snowflake_password=${{ secrets.SNOWFLAKE_PWD }}
          export AIRFLOW_VAR_snowflake_warehouse=${{ secrets.SNOWFLAKE_WH }}
          export AIRFLOW_VAR_snowflake_database=${{ secrets.SNOWFLAKE_DATABASE }}
          export AIRFLOW_VAR_snowflake_schema=${{ secrets.SNOWFLAKE_SCHEMA }}
          export AIRFLOW_VAR_snowflake_role=${{ secrets.SNOWFLAKE_ROLE }}
          export AIRFLOW_VAR_bucket_name=${{ secrets.GCS_BUCKET_NAME }}
          export AIRFLOW_VAR_taxi_trip_raw_table=${{ secrets.TAXI_TRIP_RAW_TABLE }}

      - name: Validate DAGs with Airflow
        run: |
          # Initialize Airflow database
          airflow db init
          
          # Parse all DAGs
          airflow dags list
          
          # Additional tests as needed
          # For example, you could run unit tests for your DAG tasks



