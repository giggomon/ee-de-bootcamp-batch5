name: Terraform CI/CD

on:
  push:
    branches:
      - '**'     # this matches all branches, trigger the pipeline for any branch on push
      - 'main'
  pull_request:
    branches:
      - 'main' # This ensures the workflow runs for pull requests to the main branch

jobs:
  terraform:
    name: Terraform Workflow
    runs-on: ubuntu-latest

    steps:
      # Check out the repository
      - name: Checkout repository
        uses: actions/checkout@v3

      # Ensure sample file exists
      - name: Ensure sample file exists
        run: |
          test -f resources/sample_file.txt && echo "File exists" || (echo "File missing" && exit 1)

      # Authenticate with GCP
      - name: Authenticate to GCP
        uses: google-github-actions/auth@v2
        with:
            credentials_json: ${{ secrets.GOOGLE_CREDENTIALS }}

      # Set up Terraform
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.6

      # Add environment variables for Snowflake (Remove password since we're using key-pair auth)
      - name: Set Snowflake variables
        run: |
          echo "TF_VAR_snowflake_account_name=${{ secrets.SNOWFLAKE_ACCOUNT_NAME }}" >> $GITHUB_ENV
          echo "TF_VAR_snowflake_user=${{ secrets.SNOWFLAKE_USER }}" >> $GITHUB_ENV
          echo "TF_VAR_snowflake_role=${{ secrets.SNOWFLAKE_ROLE }}" >> $GITHUB_ENV
          echo "TF_VAR_snowflake_wh=${{ secrets.SNOWFLAKE_WH }}" >> $GITHUB_ENV
          echo "TF_VAR_snowflake_database=${{ secrets.SNOWFLAKE_DATABASE }}" >> $GITHUB_ENV
          echo "TF_VAR_snowflake_schema=${{ secrets.SNOWFLAKE_SCHEMA }}" >> $GITHUB_ENV
          echo "TF_VAR_snowflake_org=${{ secrets.SNOWFLAKE_ORG }}" >> $GITHUB_ENV
          echo "TF_VAR_snowflake_storage_integration=${{ secrets.SNOWFLAKE_STORAGE_INTEGRATION }}" >> $GITHUB_ENV

      # Set Snowflake private key (using separate step to handle multiline content properly)
      - name: Set Snowflake private key
        env:
          key: |
            ${{ secrets.SNOWFLAKE_PRIVATE_KEY }}
        run: |
          echo $key
          echo "TF_VAR_snowflake_private_key=$key" >> $GITHUB_ENV

      # Set the environment variable for sample_file_name
      - name: Set Terraform Variables
        run: | 
          echo "TF_VAR_sample_file_name=${{ secrets.SAMPLE_FILE_NAME }}" >> $GITHUB_ENV
          echo "TF_VAR_taxi_trip_raw_table=${{ secrets.TAXI_TRIP_RAW_TABLE }}" >> $GITHUB_ENV
          echo "TF_VAR_taxi_trip_staging_table=${{ secrets.TAXI_TRIP_STAGE_TABLE}}" >> $GITHUB_ENV


      # Debug Variables
      - name: Debug variables
        run: echo "Sample file name:$TF_VAR_sample_file_name"

      # Terraform Init with backend auth
      - name: Terraform Init with GCS backend
        run: |
          echo '${{ secrets.GOOGLE_CREDENTIALS }}' > /tmp/account.json
          export GOOGLE_APPLICATION_CREDENTIALS=/tmp/account.json
          terraform -chdir=terraform init \
            -backend-config="bucket=${{ secrets.GCS_BUCKET_NAME }}" \
            -backend-config="prefix=terraform/state" \
            -backend-config="credentials=/tmp/account.json"
        shell: bash

      # Terraform Validate
      - name: Terraform Validate
        run: terraform -chdir=terraform validate

      # Terraform Format (Optional)
      - name: Terraform Format
        run: terraform -chdir=terraform fmt -check

      # Terraform Plan
      - name: Terraform Plan
        run: terraform -chdir=terraform plan

      # GCS Bucket status (only if it exists) checks
      - name: Import existing GCS bucket if it exists
        run: |
          BUCKET_NAME="${{ secrets.GCS_BUCKET_NAME }}"
          
          # Check if it's already in the state
          if ! terraform -chdir=terraform state list | grep -q "google_storage_bucket.landing_bucket"; then
            terraform -chdir=terraform import google_storage_bucket.landing_bucket "$BUCKET_NAME"
          else
            echo "Bucket already managed by Terraform, skipping import."
          fi

      # Snowflake Schema (only if it exists) checks
      - name: Import existing Snowflake schema if it exists
        run: |
          SCHEMA_ID="${{ secrets.SNOWFLAKE_DATABASE }}.${{ secrets.SNOWFLAKE_SCHEMA }}"
      
          # Check if schema is already managed
          if terraform -chdir=terraform state list | grep -q "snowflake_schema.schema"; then
            echo "Schema already managed by Terraform, skipping import."
          else
            terraform -chdir=terraform import snowflake_schema.schema "$SCHEMA_ID" || echo "Schema not found, will be created by Terraform"
          fi

      # Snowflake File format (only if it exists) checks
      - name: Import existing Snowflake file format if it exists
        run: |
          FILE_FORMAT_ID="${{ secrets.SNOWFLAKE_DATABASE }}|${{ secrets.SNOWFLAKE_SCHEMA }}|CSV_FORMAT"
      
          if terraform -chdir=terraform state list | grep -q "snowflake_file_format.csv_format"; then
            echo "File format already managed by Terraform, skipping import."
          else
            terraform -chdir=terraform import snowflake_file_format.csv_format "$FILE_FORMAT_ID" || echo "File format not found, will be created"
          fi

      # Terraform Apply (Only on Pushes)
      - name: Terraform Apply
        run: |
          if ! gcloud storage buckets describe gs://${{ secrets.GCS_BUCKET_NAME }} --format="value(name)" ; then
            echo "Bucket does not exist. Creating all resources..."
            terraform -chdir=terraform apply -var-file="terraform.tfvars" -auto-approve
          else
            echo "Bucket exists. Applying other changes..."
            terraform -chdir=terraform apply -var-file="terraform.tfvars" -auto-approve
          fi

  dbt:
    runs-on: ubuntu-latest
    needs: terraform  # <-- This ensures Terraform finishes first
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      # Add Snowflake environment variables for dbt
      - name: Set dbt environment variables
        run: |
          echo "SNOWFLAKE_ACCOUNT=${{ secrets.SNOWFLAKE_ACCOUNT_NAME }}" >> $GITHUB_ENV
          echo "SNOWFLAKE_USER=${{ secrets.SNOWFLAKE_USER }}" >> $GITHUB_ENV
          echo "SNOWFLAKE_PRIVATE_KEY=${{ secrets.SNOWFLAKE_PRIVATE_KEY }}" >> $GITHUB_ENV
          echo "SNOWFLAKE_ROLE=${{ secrets.SNOWFLAKE_ROLE }}" >> $GITHUB_ENV
          echo "SNOWFLAKE_WAREHOUSE=${{ secrets.SNOWFLAKE_WH }}" >> $GITHUB_ENV
          echo "SNOWFLAKE_DATABASE=${{ secrets.SNOWFLAKE_DATABASE }}" >> $GITHUB_ENV
          echo "SNOWFLAKE_SCHEMA=${{ secrets.SNOWFLAKE_SCHEMA }}" >> $GITHUB_ENV

      - name: Install dbt dependencies
        run: |
          python -m pip install --upgrade pip
          pip install dbt-snowflake  # Or your adapter, e.g., dbt-postgres

      - name: Install dbt packages
        working-directory: dbt_project
        run: dbt deps  # <--- This pulls dbt-expectations and others from packages.yml

      - name: Run dbt debug (optional sanity check)
        working-directory: dbt_project
        run: dbt debug

      - name: Run dbt tests
        working-directory: dbt_project
        run: dbt test

      - name: Run dbt models
        working-directory: dbt_project
        run: dbt run

  airflow-test:
    name: Test Airflow DAGs
    runs-on: ubuntu-latest
    needs: terraform  # This ensures the Terraform job completes first

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install apache-airflow==2.11.0
          pip install apache-airflow-providers-google==10.15.0
          pip install snowflake-connector-python>=3.0.0
          pip install apache-airflow-providers-snowflake

      - name: Test DAG syntax
        run: |
          # Test each DAG file for syntax errors
          for file in airflow/dags/*.py; do
            echo "Testing $file..."
            python -c "import sys; from airflow.models.dag import DAG; from airflow.utils.dag_cycle_tester import check_cycle; d = __import__('$file'.replace('/', '.').replace('.py', '')).dag; check_cycle(d); print('Syntax OK')"
          done

      - name: Configure Airflow Environment Variables
        run: |
          # Add Snowflake variables that your DAGs expect
          export AIRFLOW__CORE__UNIT_TEST_MODE=True
          export AIRFLOW_VAR_snowflake_account=${{ secrets.SNOWFLAKE_ACCOUNT_NAME }}
          export AIRFLOW_VAR_snowflake_user=${{ secrets.SNOWFLAKE_USER }}
          export AIRFLOW_VAR_snowflake_private_key=${{ secrets.SNOWFLAKE_PRIVATE_KEY }}
          export AIRFLOW_VAR_snowflake_warehouse=${{ secrets.SNOWFLAKE_WH }}
          export AIRFLOW_VAR_snowflake_database=${{ secrets.SNOWFLAKE_DATABASE }}
          export AIRFLOW_VAR_snowflake_schema=${{ secrets.SNOWFLAKE_SCHEMA }}
          export AIRFLOW_VAR_snowflake_role=${{ secrets.SNOWFLAKE_ROLE }}
          export AIRFLOW_VAR_bucket_name=${{ secrets.GCS_BUCKET_NAME }}
          export AIRFLOW_VAR_taxi_trip_raw_table=${{ secrets.TAXI_TRIP_RAW_TABLE }}

      - name: Validate DAGs with Airflow
        run: |
          # Initialize Airflow database
          airflow db init
          
          # Parse all DAGs
          airflow dags list
